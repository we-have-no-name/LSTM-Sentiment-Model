{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import pickle\n",
    "import csv, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "import winsound\n",
    "import re \n",
    "from IPython import display\n",
    "import importlib\n",
    "import html, string\n",
    "import preprocess_twitter\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(preprocess_twitter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('config.json') as json_config_file:\n",
    "    user_config = json.load(json_config_file)\n",
    "    data_path = user_config.get('data_path', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "change user config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# user_config = {}\n",
    "# user_config['data_path'] = r\"<your_path_here>\"\n",
    "\n",
    "# with open('config.json', 'w') as json_config_file:\n",
    "#     json.dump(user_config, json_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size=int(1.2e6)\n",
    "embedding_dim=200\n",
    "assumed_max_length=70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_dict_file = open(os.path.join(data_path, r\"d\" + str(embedding_dim) +\"_word_embedding\", \"word_dict.pickle\"), 'rb')\n",
    "word_dict = pickle.load(word_dict_file)\n",
    "word_dict2_file = open(os.path.join(data_path, r\"d\" + str(embedding_dim) +\"_word_embedding\", \"word_dict2.pickle\"), 'rb')\n",
    "word_dict2 = pickle.load(word_dict2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_embedding_file = open(os.path.join(data_path, r\"d\" + str(embedding_dim) +\"_word_embedding\", \"word_embeddings_ndarray.pickle\"), 'rb')\n",
    "np_embedding = pickle.load(word_embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_indexed = []\n",
    "sentiments = []\n",
    "sentiments_lists = []\n",
    "def remove_rt(txt):\n",
    "    txt = txt.replace('RT','')\n",
    "    return txt\n",
    "    \n",
    "def remove_link(txt):\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', txt)\n",
    "    for i in range(len(urls)):\n",
    "        txt=txt.replace(urls[i],'')\n",
    "    return txt\n",
    "    \n",
    "def remove_mentions(txt):\n",
    "    mentions = re.findall(r'(?<=\\W)[@]\\S*', txt)\n",
    "    for i in range(len(mentions)):\n",
    "        txt=txt.replace(mentions[i],'')\n",
    "    return txt\n",
    "    \n",
    "def remove_repeated_chars(txt):\n",
    "    repeated_char= re.findall(r'((\\w)\\2{2,})',txt)\n",
    "    for i  in range(len(repeated_char)):\n",
    "        txt = txt.replace(repeated_char[i][0],repeated_char[i][1])\n",
    "    return txt\n",
    "    \n",
    "def modify_lowercase_uppercase(txt):\n",
    "    text = txt.split(' ')\n",
    "    for j in range(len(text)):\n",
    "        if not(text[j].isupper()) and not(text[j].islower()):\n",
    "            text[j] = text[j].lower()\n",
    "    tweet = ' '.join(text )\n",
    "    return tweet\n",
    "\n",
    "def modify_all_lowercase(txt):\n",
    "    return txt.lower()\n",
    "\n",
    "def unescape_html(txt):\n",
    "    return html.unescape(txt)\n",
    "\n",
    "def split_punctuations(txt):\n",
    "    sub0 = re.sub(r\" *([^\\w <>']) *\", r' \\1 ', txt)\n",
    "    return re.sub(r\"(((^|(?<=\\s))')|('($|(?=\\s))))\", r' \\1 ', sub0)\n",
    "#     return re.sub(r\" *([^\\w ]) *\", r' \\1 ', txt)\n",
    "\n",
    "def split_others(text):\n",
    "    text = re.sub(r\" *([’.+…]) *\", r' \\1 ', text)\n",
    "    return text\n",
    "\n",
    "def multiclass_sentiment(line):\n",
    "    sentiment_list = []\n",
    "    for i in line[2:]:\n",
    "        if i == '' or not i.isdigit() : continue\n",
    "        sentiment_list.append(int(i) - 1)\n",
    "    return sentiment_list\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "def tweets_list_final(file_name, encoding):\n",
    "    tweets_file = open(file_name,\"r\",encoding=encoding)\n",
    "    tweets_csv_reader = csv.reader(tweets_file)\n",
    "    init_tweet=np.array([-2 for _ in range(assumed_max_length)])\n",
    "    for line in tweets_csv_reader:\n",
    "        #if line == [] : continue\n",
    "        if len(line)<2 : continue\n",
    "        try :\n",
    "            sent = int(line[2])\n",
    "            sentiments.append(sent - 1)\n",
    "            sentiments_lists.append(multiclass_sentiment(line)) \n",
    "            tweet_np = np.copy(init_tweet)\n",
    "            tweet = line[1]\n",
    "            tweets_unprocessed.append(tweet)\n",
    "            \n",
    "#             tweet1 = remove_rt(tweet)\n",
    "#             tweet1 = remove_link(tweet1)\n",
    "#             tweet1 = remove_mentions(tweet1)\n",
    "#             tweet1 = remove_repeated_chars(tweet1)\n",
    "# #             tweet1 = modify_lowercase_uppercase(tweet1) \n",
    "#             tweet1 = modify_all_lowercase(tweet1)\n",
    "#             tweet1 = unescape_html(tweet1)\n",
    "#             tweet1 = split_punctuations(tweet1)\n",
    "#             tweet1 = split_punctuations(tweet1)\n",
    "#             tweet = tweet1.split()\n",
    "        \n",
    "            tweet1=preprocess_twitter.tokenize(tweet)\n",
    "            tweet1 = unescape_html(tweet1)\n",
    "            tweet1 = split_others(tweet1)\n",
    "            tweet1= tokenizer.tokenize(tweet1)\n",
    "            tweet = tweet1\n",
    "\n",
    "            tweets.append(tweet)\n",
    "            for i in range(len(tweet)) :\n",
    "#                 tweet_np[i] = word_dict.get(tweet[i],-1)\n",
    "                tweet_np[i] = word_dict2.get(tweet[i],-1)\n",
    "                if(tweet_np[i]==-1 and (tweet[i]==' ' or tweet[i]=='T')): print(tweet1)\n",
    "\n",
    "            tweets_indexed.append(tweet_np)\n",
    "        except ValueError as err: pass\n",
    "    tweets_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "tweets_unprocessed = []\n",
    "tweets_indexed = []\n",
    "sentiments = []\n",
    "tweets_list_final(os.path.join(data_path, \"data_set0.csv\"), \"utf-8-sig\")\n",
    "tweets_list_final(os.path.join(data_path, \"data_set1.csv\"), \"utf-8-sig\")\n",
    "#tweets_list_final(os.path.join(data_path, \"data_set_a0.csv\"), \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_np = np.array(tweets_indexed)\n",
    "sents_np = np.array(sentiments, np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unmatched_words=tweets_np==-1\n",
    "all_words=tweets_np>=-1\n",
    "print('unmatched words:', round(np.count_nonzero(unmatched_words)/np.count_nonzero(all_words), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unknown_words = set()\n",
    "unknown_words_l = []\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    for j in range(tweets_np.shape[1]):\n",
    "        if tweets_np[i,j]==-1:\n",
    "            unknown_words.add(tweets[i][j])\n",
    "            unknown_words_l.append(tweets[i][j])\n",
    "print('Unknown words:')\n",
    "# unknown_words\n",
    "unknown_words_counts = []\n",
    "unknown_words_dl = list(unknown_words)\n",
    "for i in range(len(unknown_words_dl)):\n",
    "    unknown_words_counts.append([unknown_words_l.count(unknown_words_dl[i]), unknown_words_dl[i]])\n",
    "sorted(unknown_words_counts, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(assumed_max_length):\n",
    "    if np.all(tweets_np[:,i]==-2):\n",
    "        max_length=i+1; break; \n",
    "print('max_length:', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_embedded = np.zeros((tweets_np.shape[0], tweets_np.shape[1], embedding_dim))\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    tweet=tweets_np[i]\n",
    "    for j in range(max_length):\n",
    "        word_index=tweets_np[i,j]\n",
    "        if word_index == -1: continue\n",
    "        if word_index == -2: break\n",
    "        tweets_embedded[i,j]=np_embedding[word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('data set size:', len(sents_np))\n",
    "print('sent counts:', np.bincount(sents_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_units = 200\n",
    "num_layers = 1\n",
    "batch_size = 20\n",
    "classes = 8\n",
    "num_steps = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(0)\n",
    "# embedding = tf.Variable(tf.constant(0, dtype=tf.float16, shape=(vocab_size, embedding_dim)), trainable=False, name='embedding')\n",
    "inputs = tf.placeholder(tf.float32, (batch_size, num_steps, embedding_dim))\n",
    "targets = tf.placeholder(tf.int32, (batch_size,))\n",
    "\n",
    "softmax_w = tf.Variable(tf.random_uniform((num_units, classes), 0.0001, 0.001))\n",
    "softmax_b = tf.Variable(tf.random_uniform((classes,), 0.0001, 0.001))\n",
    "\n",
    "cell = rnn.BasicLSTMCell(num_units)\n",
    "tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=.7)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "all_outputs, final_states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "outputs = all_outputs[:,-1]\n",
    "\n",
    "targets_oh = tf.one_hot(targets, classes, on_value=1, off_value=0)\n",
    "\n",
    "logits = tf.matmul(outputs, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)\n",
    "results = tf.argmax(probs, 1)\n",
    "\n",
    "# losses=tf.reduce_sum(tf.square(tf.subtract(tf.cast(targets_oh, tf.float16), tf.cast(probs, tf.float16))))\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits)\n",
    "\n",
    "opt = tf.train.AdamOptimizer()\n",
    "opt_op = opt.minimize(losses)\n",
    "\n",
    "fw=tf.summary.FileWriter('TFSums', tf.get_default_graph())\n",
    "fw.flush()\n",
    "# graph_runs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iters=10; max_train=1200; max_test = 300\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, os.path.join(data_path, \"d\"+ str(embedding_dim), \"_word_embedding\", \"TF_Variables\")\n",
    "\n",
    "train_baseline=np.round(np.max(np.bincount(sents_np[:max_train]))/max_train, 3)\n",
    "test_baseline=np.round(np.max(np.bincount(sents_np[max_train:max_train+max_test]))/max_test, 3)\n",
    "\n",
    "x_axis=[]\n",
    "\n",
    "all_accs = np.zeros(iters)\n",
    "all_accs_sampled = np.zeros(iters//5+1)\n",
    "tests_accs = np.zeros(iters//5+1)\n",
    "for i in range(iters):\n",
    "    checkpoint = i if i%5 == 0 or i==iters-1 else None\n",
    "    iter_train_result = np.zeros(max_train); final_train_probs = np.zeros((max_train, classes))\n",
    "    iter_test_result = np.zeros(max_test); final_test_probs = np.zeros((max_test, classes))\n",
    "    for j in range(int(max_train/batch_size)):\n",
    "        np_inputs = tweets_embedded[j*batch_size:(j+1)*batch_size, :max_length]\n",
    "        np_targets = sents_np[j*batch_size:(j+1)*batch_size]\n",
    "        if checkpoint and i==iters-1:\n",
    "            _, np_probs, np_results = sess.run([opt_op, probs, results], feed_dict={inputs: np_inputs, targets: np_targets})\n",
    "            iter_train_result[j*batch_size:(j+1)*batch_size] = np_results\n",
    "            final_train_probs[j*batch_size:(j+1)*batch_size] = np_probs\n",
    "        elif checkpoint:\n",
    "            _, np_results = sess.run([opt_op, results], feed_dict={inputs: np_inputs, targets: np_targets})\n",
    "            iter_train_result[j*batch_size:(j+1)*batch_size] = np_results\n",
    "        else:\n",
    "            _ = sess.run([opt_op], feed_dict={inputs: np_inputs, targets: np_targets})\n",
    "\n",
    "    if checkpoint is None : continue\n",
    "    if i==iters-1: checkpoint+=1\n",
    "    checkpoint//=5\n",
    "#     checks = iter_train_result==sents_np[:max_train] \n",
    "    checks = np.fromiter(map(lambda i,j :i in j, iter_train_result, sentiments_lists[:max_train]), dtype=np.bool)\n",
    "    all_accs[i] = np.count_nonzero(checks)/checks.size\n",
    "    \n",
    "    #update plot\n",
    "    all_accs_sampled[checkpoint] = all_accs[i]\n",
    "    x_axis.append(checkpoint*5)\n",
    "#     sys.stdout.write('\\r'); sys.stdout.write('      '); sys.stdout.write('\\r')\n",
    "#     print(i, end='')\n",
    "    plt.gca().cla() \n",
    "    plt.plot(x_axis,all_accs_sampled[:(checkpoint)+1], label=\"train(bl=\" + str(train_baseline) + \")\")\n",
    "    for test in range(int(max_test/batch_size)):\n",
    "        np_test_inputs = tweets_embedded[max_train + (test*batch_size):max_train + ((test+1)*batch_size), :num_steps]\n",
    "        np_test_targets = sents_np[max_train + (test*batch_size):max_train + ((test+1)*batch_size)]\n",
    "        if i==iters-1:\n",
    "            np_test_results, np_test_probs = sess.run([results, probs], feed_dict={inputs:  np_test_inputs, targets: np_test_targets})\n",
    "            final_test_probs[(test*batch_size):((test+1)*batch_size)] = np_test_probs\n",
    "        else:\n",
    "            np_test_results, = sess.run([ results], feed_dict={inputs:  np_test_inputs, targets: np_test_targets})\n",
    "        iter_test_result[(test*batch_size):((test+1)*batch_size)] = np_test_results\n",
    "#     checks = iter_test_result==sents_np[max_train:max_train + max_test] \n",
    "    checks = np.fromiter(map(lambda i,j :i in j, iter_test_result, sentiments_lists[max_train:max_train + max_test]), dtype=np.bool)\n",
    "    tests_accs[checkpoint] = np.count_nonzero(checks)/checks.size\n",
    "    plt.plot(x_axis,tests_accs[:(checkpoint)+1],label=\"test(bl=\" + str(test_baseline) + \")\")\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "# sys.stdout.write('\\r'); sys.stdout.write('      '); sys.stdout.write('\\r')\n",
    "\n",
    "winsound.Beep(2500,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('train_acc:', all_accs[-1], '  test_acc:', tests_accs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Wrong results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_map=['Happy','Love','Hopeful','Neutral','Angry','Hopeless','Hate','Sad']\n",
    "for i in range(max_test):\n",
    "          if checks[i]==False:\n",
    "            print(' '.join(tweets[max_train+i]), '\\n>' , sent_map[int(iter_test_result[i])], \">>\", [sent_map[j] for j in sentiments_lists[max_train+i]], '>>>', [float('{:.3}'.format(k)) for k in final_test_probs[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unprocessed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sent_map=['Happy','Love','Hopeful','Neutral','Angry','Hopeless','Hate','Sad']\n",
    "for i in range(max_test):\n",
    "          if checks[i]==False:\n",
    "            print(tweets_unprocessed[max_train+i], '\\n>' , sent_map[int(iter_test_result[i])], \">>\", [sent_map[j] for j in sentiments_lists[max_train+i]], '>>>', [float('{:.3}'.format(k)) for k in final_test_probs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(data_path, 'Sessions', 'i1500_CL_41TA', 'i1500_CL_41TA'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
