{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import pickle\n",
    "import csv, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "import sys, os, time, shutil\n",
    "import winsound\n",
    "import re \n",
    "from IPython import display\n",
    "import importlib\n",
    "import html, string\n",
    "import preprocess_twitter\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "reload preprocess_twitter.py if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(preprocess_twitter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "read user config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('config.json') as json_config_file:\n",
    "    user_config = json.load(json_config_file)\n",
    "    data_path = user_config.get('data_path', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "change user config if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# user_config = {}\n",
    "# user_config['data_path'] = r\"<your_path_here>\"\n",
    "\n",
    "# with open('config.json', 'w') as json_config_file:\n",
    "#     json.dump(user_config, json_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size=int(1.2e6)\n",
    "embedding_dim=200\n",
    "assumed_max_length=70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "read the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_dict_file = open(os.path.join(data_path, r\"d\" + str(embedding_dim) +\"_word_embedding\", \"word_dict.pickle\"), 'rb')\n",
    "word_dict = pickle.load(word_dict_file)\n",
    "word_dict2_file = open(os.path.join(data_path, r\"d\" + str(embedding_dim) +\"_word_embedding\", \"word_dict2.pickle\"), 'rb')\n",
    "word_dict2 = pickle.load(word_dict2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_embedding_file = open(os.path.join(data_path, r\"d\" + str(embedding_dim) +\"_word_embedding\", \"word_embeddings_ndarray.pickle\"), 'rb')\n",
    "np_embedding = pickle.load(word_embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_rt(txt):\n",
    "    txt = txt.replace('RT','')\n",
    "    return txt\n",
    "    \n",
    "def remove_link(txt):\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', txt)\n",
    "    for i in range(len(urls)):\n",
    "        txt=txt.replace(urls[i],'')\n",
    "    return txt\n",
    "    \n",
    "def remove_mentions(txt):\n",
    "    mentions = re.findall(r'(?<=\\W)[@]\\S*', txt)\n",
    "    for i in range(len(mentions)):\n",
    "        txt=txt.replace(mentions[i],'')\n",
    "    return txt\n",
    "    \n",
    "def remove_repeated_chars(txt):\n",
    "    repeated_char= re.findall(r'((\\w)\\2{2,})',txt)\n",
    "    for i  in range(len(repeated_char)):\n",
    "        txt = txt.replace(repeated_char[i][0],repeated_char[i][1])\n",
    "    return txt\n",
    "    \n",
    "def modify_lowercase_uppercase(txt):\n",
    "    txt = txt.split(' ')\n",
    "    for j in range(len(txt)):\n",
    "        if not(txt[j].isupper()) and not(txt[j].islower()):\n",
    "            txt[j] = txt[j].lower()\n",
    "    tweet = ' '.join(txt )\n",
    "    return tweet\n",
    "\n",
    "def modify_all_lowercase(txt):\n",
    "    return txt.lower()\n",
    "\n",
    "def unescape_html(txt):\n",
    "    return html.unescape(txt)\n",
    "\n",
    "def split_punctuations(txt):\n",
    "    txt = re.sub(r\" *([^\\w ']|( '|' |^'|'$)) *\", r' \\1 ', txt)\n",
    "#     txt=re.sub(r\"(((^|(?<=\\s))')|('($|(?=\\s))))\", r' \\1 ', sub0)\n",
    "#     txt=re.sub(r\" *([^\\w ]) *\", r' \\1 ', txt)\n",
    "    return txt\n",
    "\n",
    "def split_emojis(txt):\n",
    "    emojis='[\\U0001F601-\\U0001F64F\\U00002702-\\U000027B0\\U0001F680-\\U0001F6C0\\U000024C2-\\U0001F251\\U0001F600-\\U0001F636\\U0001F681-\\U0001F6C5\\U0001F30D-\\U0001F567]'\n",
    "    return re.sub(r\" *({}) *\".format(emojis), r' \\1 ', txt)\n",
    "\n",
    "def replace_emojis(txt):\n",
    "    txt=re.sub('[\\U0000FE00-\\U0000FE0F]', '', txt) #remove variation selectors\n",
    "    txt=re.sub('[\\U0001F3FB-\\U0001F3FF]', '', txt) #remove color tones\n",
    "    smile_ug = '[\\U0001F603\\U0001F604\\U0001F600]'\n",
    "    lolface_ug = '[\\U0001F602\\U0001F606]'\n",
    "    sadface_ug = '[\\U0001F614\\U0001F616\\U0001F622\\U0001F625\\U0001F629\\U0001F62D\\U0001F630\\U0001F63F]'\n",
    "    neutralface_ug = '[\\U0001F610]'\n",
    "    heart_ug = '[\\U0001F60D\\U0001F618\\U0001F63B\\U00002764\\U0001F491-\\U0001F49F]'\n",
    "    txt = re.sub(smile_ug, \" ᐸsmileᐳ \", txt)\n",
    "    txt = re.sub(lolface_ug, \" ᐸlolfaceᐳ \", txt)\n",
    "    txt = re.sub(sadface_ug, \" ᐸsadfaceᐳ \", txt)\n",
    "    txt = re.sub(neutralface_ug, \" ᐸneutralfaceᐳ \", txt)\n",
    "    txt = re.sub(heart_ug, \" ᐸheartᐳ \", txt)\n",
    "    return txt\n",
    "\n",
    "def split_others(txt):\n",
    "    emojis='[\\U0001F601-\\U0001F64F\\U00002702-\\U000027B0\\U0001F680-\\U0001F6C0\\U000024C2-\\U0001F251\\U0001F600-\\U0001F636\\U0001F681-\\U0001F6C5\\U0001F30D-\\U0001F567]'\n",
    "    txt = re.sub(r\" *(( '|' |^'|'$)|[’.+…”“*]|{}) *\".format(emojis), r' \\1 ', txt)\n",
    "    return txt\n",
    "\n",
    "def multiclass_sentiment(line):\n",
    "    sentiment_list = []\n",
    "    for i in line[2:]:\n",
    "        if i == '' or not i.isdigit() : continue\n",
    "        sentiment_list.append(int(i) - 1)\n",
    "    return sentiment_list\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "def tweets_list_final(file_name, encoding):\n",
    "    global count1, count2, count21, count3\n",
    "    tweets_file = open(file_name,\"r\",encoding=encoding)\n",
    "    tweets_csv_reader = csv.reader(tweets_file)\n",
    "    init_tweet=np.array([-2 for _ in range(assumed_max_length)])\n",
    "    for line in tweets_csv_reader:\n",
    "        if len(line)<2 or line[1]=='' : continue\n",
    "        for i in range(len(line[2:])):\n",
    "            try:\n",
    "                sent = int(line[2+i])\n",
    "                break\n",
    "            except ValueError:\n",
    "                sent=0\n",
    "        if sent==0: continue\n",
    "        sentiments.append(sent - 1)\n",
    "        sentiments_lists.append(multiclass_sentiment(line)) \n",
    "        \n",
    "        tweet_np = np.copy(init_tweet)\n",
    "        tweet = line[1]\n",
    "        tweets_unprocessed.append(tweet)\n",
    "\n",
    "        tweet1 = preprocess_twitter.tokenize(tweet)\n",
    "        tweet1 = unescape_html(tweet1)\n",
    "        tweet1 = split_punctuations(tweet1)\n",
    "        tweet1 = split_emojis(tweet1)\n",
    "        tweet1 = replace_emojis(tweet1)\n",
    "#             tweet1 = split_others(tweet1)\n",
    "        tweet1= tokenizer.tokenize(tweet1)\n",
    "        tweet = tweet1\n",
    "\n",
    "        tweets.append(tweet)\n",
    "        for i in range(len(tweet)) :\n",
    "#                 tweet_np[i] = word_dict.get(tweet[i],-1)\n",
    "            tweet_np[i] = word_dict2.get(tweet[i],-1)\n",
    "\n",
    "        tweets_indexed.append(tweet_np)\n",
    "        \n",
    "    tweets_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "load tweets files and create a list for word-ids and a list for sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classes = 8\n",
    "tweets = []\n",
    "tweets_unprocessed = []\n",
    "tweets_indexed = []\n",
    "sentiments = []\n",
    "sentiments_lists = []\n",
    "tweets_list_final(os.path.join(data_path, \"data_set0.csv\"), \"utf-8-sig\")\n",
    "tweets_list_final(os.path.join(data_path, \"data_set1.csv\"), \"utf-8-sig\")\n",
    "tweets_list_final(os.path.join(data_path, \"data_set2.csv\"), \"utf-8-sig\")\n",
    "#tweets_list_final(os.path.join(data_path, \"data_set_a0.csv\"), \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "convert the list to np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_np = np.array(tweets_indexed)\n",
    "sents_np = np.array(sentiments, np.int16)\n",
    "sents_mh_np = np.zeros((tweets_np.shape[0], classes), np.bool) #mh: multihot\n",
    "sents_sc_np = np.zeros((tweets_np.shape[0], classes), np.float32) #sc: soft classes\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    sents_mh_np[i, sentiments_lists[i]] = 1\n",
    "priority_probs=[np.array([1]), np.array([2/3, 1/3]), np.array([3/6, 2/6, 1/6])]\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    sc_indices = sentiments_lists[i]\n",
    "    sents_sc_np[i, sc_indices] = priority_probs[len(sc_indices)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "some statistics on words that couldn't be embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unmatched_words=tweets_np==-1\n",
    "all_words=tweets_np>=-1\n",
    "print('unmatched words:', round(np.count_nonzero(unmatched_words)/np.count_nonzero(all_words), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unknown_words = set()\n",
    "unknown_words_l = []\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    for j in range(tweets_np.shape[1]):\n",
    "        if tweets_np[i,j]==-1:\n",
    "            unknown_words.add(tweets[i][j])\n",
    "            unknown_words_l.append(tweets[i][j])\n",
    "print('Unknown words:')\n",
    "# unknown_words\n",
    "unknown_words_counts = []\n",
    "unknown_words_dl = list(unknown_words)\n",
    "for i in range(len(unknown_words_dl)):\n",
    "    unknown_words_counts.append([unknown_words_l.count(unknown_words_dl[i]), unknown_words_dl[i]])\n",
    "for item in sorted(unknown_words_counts, reverse=True):\n",
    "    print('{}: {}'.format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "get the maximum tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(assumed_max_length):\n",
    "    if np.all(tweets_np[:,i]==-2):\n",
    "        max_length=i+1; break; \n",
    "print('max_length:', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "convert word-ids into the equivalent word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_embedded = np.zeros((tweets_np.shape[0], tweets_np.shape[1], embedding_dim))\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    tweet=tweets_np[i]\n",
    "    for j in range(max_length):\n",
    "        word_index=tweets_np[i,j]\n",
    "        if word_index == -1: continue\n",
    "        if word_index == -2: break\n",
    "        tweets_embedded[i,j]=np_embedding[word_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('data set size:', len(sents_np))\n",
    "print('sent counts:', np.bincount(sents_np))\n",
    "top_3_baseline = sum(sorted(np.bincount(sents_np), reverse=True)[:3])/sum(np.bincount(sents_np))\n",
    "print('top 3-classses per all: {:.3}'.format(top_3_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def iters_stats():\n",
    "    latest_acc = 'train_acc: {:.3}\\ttest_acc: {:.3}\\n'.format(train_accs[-1], test_accs[-1])\n",
    "    top_acc = 'top train_acc: {:.3}\\ttop test_acc: {:.3}\\n'.format(max(train_accs), max(test_accs))\n",
    "    pass_test = 'passing test examples: {:.3}\\n'.format(np.mean(tests_acc>0.5))\n",
    "    test_acc2_o1 = np.argsort(iter_test_probs)[:,1] == sents_np[max_train:max_train + max_test]\n",
    "    test_acc2_o2 = np.sum(np.minimum(iter_test_probs, sents_sc_np[max_train:max_train + max_test]), 1)\n",
    "    test_acc2_f = np.mean(np.maximum(test_acc2_o1, test_acc2_o2))\n",
    "    test_acc2 = 'test_acc2: {:.3}\\n'.format(test_acc2_f)\n",
    "    std_dev = 'standard deviation: {:.3}\\n'.format(np.mean(np.std(iter_test_result, 0)))\n",
    "    ex_time = \"execution time: {:.3f} seconds\".format(end_time - start_time)\n",
    "    return (latest_acc + top_acc + pass_test + test_acc2 + std_dev + ex_time)\n",
    "    \n",
    "def iters_plot():\n",
    "    set_plot()\n",
    "    plt.show()\n",
    "    \n",
    "def set_plot():\n",
    "    plt.gca().cla() \n",
    "    plt.plot(x_axis,train_accs[:(checkpoint)+1], label=\"train(bl=\" + str(train_baseline) + \")\")\n",
    "    plt.plot(x_axis,test_accs[:(checkpoint)+1], label=\"test (bl=\" + str(test_baseline) + \")\")\n",
    "    plt.title('Accuracy at iterations ({}:{})'.format(x_axis[0], x_axis[-1]))\n",
    "    plt.legend(loc='best') #upper left    \n",
    "\n",
    "sent_map=['Happy','Love','Hopeful','Neutral','Angry','Hopeless','Hate','Sad']\n",
    "def tweets_with_results(group ='wrong', processed = False):\n",
    "    text = ''\n",
    "    for i in range(max_test):\n",
    "#         if tests_acc[i]==False:\n",
    "        item_acc = np.sum(tests_acc[i])/np.sum(sents_sc_np[max_train+i])\n",
    "        condition = True\n",
    "        if group == 'wrong': condition = item_acc<0.5\n",
    "        if group == 'correct': condition = item_acc>=0.5\n",
    "        if condition:\n",
    "            if processed: tweet_p = ' '.join(tweets[max_train+i])\n",
    "            else: tweet_p = tweets_unprocessed[max_train+i]\n",
    "            res_p=', '.join(['{}: {:.3}'.format(sent_map[l], iter_test_probs[i, l]) for l in np.argsort(iter_test_probs[i])[::-1][:3] if iter_test_probs[i, l]>0.01])\n",
    "            targets_p=', '.join([sent_map[j] for j in sentiments_lists[max_train+i]])\n",
    "#             probs_p=[float('{:.3}'.format(k)) for k in iter_test_probs[i]]\n",
    "            text += tweet_p + '\\n> ' + res_p + \" >> \" + targets_p + '\\n'\n",
    "    return text\n",
    "\n",
    "def log_iters():\n",
    "    log_folder = os.path.join(data_path, 'log', \"{}_tr{}te{}_u{}l{}do{}d{}_{}\".format(iters_label, max_train, max_test, num_units, num_layers, drop_out, embedding_dim, session_init_time))\n",
    "    set_plot()\n",
    "    if not os.path.exists(log_folder): os.makedirs(log_folder)\n",
    "    log_file_name = os.path.join(log_folder, \"i{}-{}\".format(x_axis[0], x_axis[-1]))\n",
    "    plt.savefig(log_file_name + '.svg');\n",
    "    plt.savefig(log_file_name + '.png');\n",
    "    plt.close()\n",
    "    with open(log_file_name + '.txt', 'w') as log_file: log_file.write(iters_stats())\n",
    "    with open(log_file_name + '_rejected.txt', 'w', encoding = 'utf-8') as log_file: log_file.write(tweets_with_results())\n",
    "    with open(log_file_name + '_approved.txt', 'w', encoding = 'utf-8') as log_file: log_file.write(tweets_with_results(group = 'correct'))\n",
    "    if iters==total_iters: shutil.copy2(os.path.join(os.getcwd(), 'Model Si.ipynb'), log_folder)\n",
    "        \n",
    "def save_session():\n",
    "    saver = tf.train.Saver()\n",
    "    sess_name='TA{:.3}-{:.3}_{}_tr{}te{}_u{}l{}do{}d{}'.format(test_accs[-1], max(test_accs), iters_label, max_train, max_test, num_units, num_layers, drop_out, embedding_dim)\n",
    "    sess_folder = os.path.join(data_path, 'Sessions', sess_name)\n",
    "    if not os.path.exists(sess_folder): os.makedirs(sess_folder)\n",
    "    saver.save(sess, os.path.join(sess_folder, iters_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_units = 200\n",
    "num_layers = 1\n",
    "drop_out = 0.0\n",
    "batch_size = 100\n",
    "num_steps = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(0)\n",
    "use_dropout = tf.constant(True)\n",
    "# embedding = tf.Variable(tf.constant(0, dtype=tf.float16, shape=(vocab_size, embedding_dim)), trainable=False, name='embedding')\n",
    "inputs = tf.placeholder(tf.float32, (batch_size, num_steps, embedding_dim))\n",
    "inputs_d = tf.nn.dropout(inputs, 1-drop_out)\n",
    "inputs_c = tf.cond(use_dropout, lambda: inputs_d, lambda: inputs)\n",
    "# targets = tf.placeholder(tf.int32, (batch_size,))\n",
    "targets_mc = tf.placeholder(tf.float32, (batch_size, classes))\n",
    "\n",
    "softmax_w = tf.Variable(tf.random_uniform((num_units, classes), 0.0001, 0.001))\n",
    "softmax_b = tf.Variable(tf.random_uniform((classes,), 0.0001, 0.001))\n",
    "\n",
    "cell = rnn.BasicLSTMCell(num_units)\n",
    "cell_m = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "all_outputs, final_states = tf.nn.dynamic_rnn(cell_m, inputs_c, dtype=tf.float32)\n",
    "outputs = all_outputs[:,-1]\n",
    "\n",
    "# targets_oh = tf.one_hot(targets, classes, on_value=1, off_value=0)\n",
    "\n",
    "logits = tf.matmul(outputs, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)\n",
    "results = tf.nn.top_k(probs, 3).indices\n",
    "# probs = tf.nn.sigmoid(logits)\n",
    "# results = probs>0.5\n",
    "\n",
    "# losses=tf.reduce_sum(tf.square(tf.subtract(tf.cast(targets_oh, tf.float16), tf.cast(probs, tf.float16))))\n",
    "# losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits)\n",
    "# losses = -tf.reduce_sum(targets_mc * tf.log(tf.clip_by_value(probs, 1e-10, 1.0)))\n",
    "losses=tf.reduce_sum(tf.square(tf.subtract(targets_mc, probs)))\n",
    "# losses = tf.nn.softmax_cross_entropy_with_logits(labels=targets_mc, logits=logits)\n",
    "\n",
    "opt = tf.train.AdamOptimizer()\n",
    "opt_op = opt.minimize(losses)\n",
    "\n",
    "# fw=tf.summary.FileWriter('TFSums', tf.get_default_graph())\n",
    "# fw.flush()\n",
    "# graph_runs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "iters=100;\n",
    "max_train=1600; max_test = 400;\n",
    "iters_label = \"ModelSi-PF-do0.0\"\n",
    "\n",
    "train_baseline=np.round(np.max(np.bincount(sents_np[:max_train]))/max_train, 3)\n",
    "test_baseline=np.round(np.max(np.bincount(sents_np[max_train:max_train+max_test]))/max_test, 3)\n",
    "\n",
    "train_accs = np.zeros(iters//5+1)\n",
    "test_accs = np.zeros(iters//5+1)\n",
    "iter_train_probs = np.zeros((max_train, classes))\n",
    "iter_test_probs = np.zeros((max_test, classes))\n",
    "iter_train_result = np.zeros((max_train, classes), np.bool)\n",
    "iter_test_result = np.zeros((max_test, classes), np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_iters=0; session_init_time='UTC'+time.strftime(\"%y%m%d-%H%M%S\", time.gmtime())\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, os.path.join(data_path, \"d\"+ str(embedding_dim), \"_word_embedding\", \"TF_Variables\")\n",
    "\n",
    "x_axis=[]\n",
    "for i in range(iters):\n",
    "    checkpoint = None\n",
    "    if (i+1)%5 == 0: checkpoint = (i+1)//5 \n",
    "    if i==0: checkpoint=0\n",
    "    \n",
    "    # Train the model and get train results\n",
    "    for train in range(int(max_train/batch_size)):\n",
    "        np_inputs = tweets_embedded[train*batch_size:(train+1)*batch_size, :num_steps]\n",
    "        np_targets = sents_sc_np[train*batch_size:(train+1)*batch_size]\n",
    "        if checkpoint is not None:\n",
    "            _, np_probs, np_results = sess.run([opt_op, probs, results], feed_dict={inputs: np_inputs, targets_mc: np_targets})\n",
    "#             iter_train_result[train*batch_size:(train+1)*batch_size] = np.eye(classes)[np_results.squeeze()] #one_hot\n",
    "            iter_train_result[np.arange(train*batch_size,(train+1)*batch_size)[:, np.newaxis], np_results] = True\n",
    "            iter_train_probs[train*batch_size:(train+1)*batch_size] = np_probs\n",
    "        else:\n",
    "            _ = sess.run([opt_op], feed_dict={inputs: np_inputs, targets_mc: np_targets})\n",
    "\n",
    "    # Collect and present data or go to the next iteration\n",
    "    if checkpoint is None: continue\n",
    "\n",
    "    # Get test results\n",
    "    for test in range(int(max_test/batch_size)):\n",
    "        np_test_inputs = tweets_embedded[max_train + (test*batch_size):max_train + ((test+1)*batch_size), :num_steps]\n",
    "        np_test_targets = sents_sc_np[max_train + (test*batch_size):max_train + ((test+1)*batch_size)]\n",
    "        np_test_results, np_test_probs = sess.run([results, probs], feed_dict={inputs:  np_test_inputs, targets_mc: np_test_targets, use_dropout: False})\n",
    "        iter_test_probs[(test*batch_size):((test+1)*batch_size)] = np_test_probs\n",
    "        iter_test_result[np.arange((test*batch_size),((test+1)*batch_size))[:, np.newaxis], np_test_results] = True\n",
    "        \n",
    "    # Calculate train accuracy\n",
    "#     train_acc = np.fromiter(map(lambda i,j :i in j, iter_train_result, sentiments_lists[:max_train]), dtype=np.bool)\n",
    "#     train_acc = iter_train_result == sents_mh_np[:max_train]\n",
    "#     train_acc = np.any(np.logical_and(iter_train_result, sents_mh_np[:max_train]), 1)\n",
    "    train_acc = np.sum(np.minimum(iter_train_probs, sents_sc_np[:max_train]), 1)\n",
    "    train_accs[checkpoint] = np.mean(train_acc)\n",
    "   \n",
    "    # Calculate test accuracy\n",
    "#     tests_acc = np.fromiter(map(lambda i,j :i in j, iter_test_result, sentiments_lists[max_train:max_train + max_test]), dtype=np.bool)\n",
    "#     tests_acc = iter_test_result == sents_mh_np[max_train:max_train + max_test]\n",
    "#     tests_acc = np.any(np.logical_and(iter_test_result, sents_mh_np[max_train:max_train + max_test]), 1)\n",
    "    tests_acc = np.sum(np.minimum(iter_test_probs, sents_sc_np[max_train:max_train + max_test]), 1)\n",
    "    test_accs[checkpoint] = np.mean(tests_acc)\n",
    "    \n",
    "    # Update the plot    \n",
    "    x_axis.append(total_iters + i+1)\n",
    "    plt.gca().cla() \n",
    "    plt.plot(x_axis,train_accs[:(checkpoint)+1], label=\"train(bl=\" + str(train_baseline) + \")\")\n",
    "    plt.plot(x_axis,test_accs[:(checkpoint)+1], label=\"test (bl=\" + str(test_baseline) + \")\")\n",
    "    plt.title('Accuracy at iterations ({}:{})'.format(x_axis[0], x_axis[-1]))\n",
    "    plt.legend(loc='best') #upper left\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "total_iters+=iters\n",
    "end_time=time.time()\n",
    "plt.close() #close the plot since it's already been displayed\n",
    "\n",
    "# iters_plot()\n",
    "print(iters_stats())\n",
    "log_iters()\n",
    "winsound.Beep(2500,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Wrong results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tweets_with_results())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "processed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(tweets_with_results(processed = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
