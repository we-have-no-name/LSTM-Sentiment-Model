{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import pickle\n",
    "import csv, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys, os, time\n",
    "import winsound\n",
    "import re \n",
    "from IPython import display\n",
    "import importlib\n",
    "import html, string\n",
    "import preprocess_twitter\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#importlib.reload(preprocess_twitter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('config.json') as json_config_file:\n",
    "    user_config = json.load(json_config_file)\n",
    "    data_path = user_config.get('data_path', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "change user config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# user_config = {}\n",
    "# user_config['data_path'] = r\"<your_path_here>\"\n",
    "\n",
    "# with open('config.json', 'w') as json_config_file:\n",
    "#     json.dump(user_config, json_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size=int(1.2e6)\n",
    "embedding_dim=200\n",
    "assumed_max_length=70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_dict_file = open(os.path.join(data_path, r\"d\" + str(embedding_dim) +\"_word_embedding\", \"word_dict.pickle\"), 'rb')\n",
    "word_dict = pickle.load(word_dict_file)\n",
    "word_dict2_file = open(os.path.join(data_path, r\"d\" + str(embedding_dim) +\"_word_embedding\", \"word_dict2.pickle\"), 'rb')\n",
    "word_dict2 = pickle.load(word_dict2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_embedding_file = open(os.path.join(data_path, r\"d\" + str(embedding_dim) +\"_word_embedding\", \"word_embeddings_ndarray.pickle\"), 'rb')\n",
    "np_embedding = pickle.load(word_embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_rt(txt):\n",
    "    txt = txt.replace('RT','')\n",
    "    return txt\n",
    "    \n",
    "def remove_link(txt):\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', txt)\n",
    "    for i in range(len(urls)):\n",
    "        txt=txt.replace(urls[i],'')\n",
    "    return txt\n",
    "    \n",
    "def remove_mentions(txt):\n",
    "    mentions = re.findall(r'(?<=\\W)[@]\\S*', txt)\n",
    "    for i in range(len(mentions)):\n",
    "        txt=txt.replace(mentions[i],'')\n",
    "    return txt\n",
    "    \n",
    "def remove_repeated_chars(txt):\n",
    "    repeated_char= re.findall(r'((\\w)\\2{2,})',txt)\n",
    "    for i  in range(len(repeated_char)):\n",
    "        txt = txt.replace(repeated_char[i][0],repeated_char[i][1])\n",
    "    return txt\n",
    "    \n",
    "def modify_lowercase_uppercase(txt):\n",
    "    txt = txt.split(' ')\n",
    "    for j in range(len(txt)):\n",
    "        if not(txt[j].isupper()) and not(txt[j].islower()):\n",
    "            txt[j] = txt[j].lower()\n",
    "    tweet = ' '.join(txt )\n",
    "    return tweet\n",
    "\n",
    "def modify_all_lowercase(txt):\n",
    "    return txt.lower()\n",
    "\n",
    "def unescape_html(txt):\n",
    "    return html.unescape(txt)\n",
    "\n",
    "def split_punctuations(txt):\n",
    "    txt = re.sub(r\" *([^\\w ']|( '|' |^'|'$)) *\", r' \\1 ', txt)\n",
    "#     txt=re.sub(r\"(((^|(?<=\\s))')|('($|(?=\\s))))\", r' \\1 ', sub0)\n",
    "#     txt=re.sub(r\" *([^\\w ]) *\", r' \\1 ', txt)\n",
    "    return txt\n",
    "\n",
    "def split_emojis(txt):\n",
    "    emojis='[\\U0001F601-\\U0001F64F\\U00002702-\\U000027B0\\U0001F680-\\U0001F6C0\\U000024C2-\\U0001F251\\U0001F600-\\U0001F636\\U0001F681-\\U0001F6C5\\U0001F30D-\\U0001F567]'\n",
    "    return re.sub(r\" *({}) *\".format(emojis), r' \\1 ', txt)\n",
    "\n",
    "def replace_emojis(txt):\n",
    "    txt=re.sub('[\\U0000FE00-\\U0000FE0F]', '', txt) #remove variation selectors\n",
    "    txt=re.sub('[\\U0001F3FB-\\U0001F3FF]', '', txt) #remove color tones\n",
    "    smile_ug = '[\\U0001F603\\U0001F604\\U0001F600]'\n",
    "    lolface_ug = '[\\U0001F602\\U0001F606]'\n",
    "    sadface_ug = '[\\U0001F614\\U0001F616\\U0001F622\\U0001F625\\U0001F629\\U0001F62D\\U0001F630\\U0001F63F]'\n",
    "    neutralface_ug = '[\\U0001F610]'\n",
    "    heart_ug = '[\\U0001F60D\\U0001F618\\U0001F63B\\U00002764\\U0001F491-\\U0001F49F]'\n",
    "    txt = re.sub(smile_ug, \" ᐸsmileᐳ \", txt)\n",
    "    txt = re.sub(lolface_ug, \" ᐸlolfaceᐳ \", txt)\n",
    "    txt = re.sub(sadface_ug, \" ᐸsadfaceᐳ \", txt)\n",
    "    txt = re.sub(neutralface_ug, \" ᐸneutralfaceᐳ \", txt)\n",
    "    txt = re.sub(heart_ug, \" ᐸheartᐳ \", txt)\n",
    "    return txt\n",
    "\n",
    "def split_others(txt):\n",
    "    emojis='[\\U0001F601-\\U0001F64F\\U00002702-\\U000027B0\\U0001F680-\\U0001F6C0\\U000024C2-\\U0001F251\\U0001F600-\\U0001F636\\U0001F681-\\U0001F6C5\\U0001F30D-\\U0001F567]'\n",
    "    txt = re.sub(r\" *(( '|' |^'|'$)|[’.+…”“*]|{}) *\".format(emojis), r' \\1 ', txt)\n",
    "    return txt\n",
    "\n",
    "def multiclass_sentiment(line):\n",
    "    sentiment_list = []\n",
    "    for i in line[2:]:\n",
    "        if i == '' or not i.isdigit() : continue\n",
    "        sentiment_list.append(int(i) - 1)\n",
    "    return sentiment_list\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "def tweets_list_final(file_name, encoding):\n",
    "    tweets_file = open(file_name,\"r\",encoding=encoding)\n",
    "    tweets_csv_reader = csv.reader(tweets_file)\n",
    "    init_tweet=np.array([-2 for _ in range(assumed_max_length)])\n",
    "    for line in tweets_csv_reader:\n",
    "        #if line == [] : continue\n",
    "        if len(line)<2 : continue\n",
    "        try :\n",
    "            sent = int(line[2])\n",
    "            sentiments.append(sent - 1)\n",
    "            sentiments_lists.append(multiclass_sentiment(line)) \n",
    "            tweet_np = np.copy(init_tweet)\n",
    "            tweet = line[1]\n",
    "            tweets_unprocessed.append(tweet)\n",
    "            \n",
    "#             tweet1 = remove_rt(tweet)\n",
    "#             tweet1 = remove_link(tweet1)\n",
    "#             tweet1 = remove_mentions(tweet1)\n",
    "#             tweet1 = remove_repeated_chars(tweet1)\n",
    "#               tweet1 = modify_lowercase_uppercase(tweet1) \n",
    "#             tweet1 = modify_all_lowercase(tweet1)\n",
    "#             tweet1 = unescape_html(tweet1)\n",
    "#             tweet1 = split_punctuations(tweet1)\n",
    "#             tweet1 = split_punctuations(tweet1)\n",
    "#             tweet = tweet1.split()\n",
    "        \n",
    "            tweet1=preprocess_twitter.tokenize(tweet)\n",
    "            tweet1 = unescape_html(tweet1)\n",
    "            tweet1 = split_punctuations(tweet1)\n",
    "            tweet1 = split_emojis(tweet1)\n",
    "            tweet1 = replace_emojis(tweet1)\n",
    "#             tweet1 = split_others(tweet1)\n",
    "            tweet1= tokenizer.tokenize(tweet1)\n",
    "            tweet = tweet1\n",
    "\n",
    "            tweets.append(tweet)\n",
    "            for i in range(len(tweet)) :\n",
    "#                 tweet_np[i] = word_dict.get(tweet[i],-1)\n",
    "                tweet_np[i] = word_dict2.get(tweet[i],-1)\n",
    "                if(tweet_np[i]==-1 and (tweet[i]==' ' or tweet[i]=='T')): print(tweet1)\n",
    "\n",
    "            tweets_indexed.append(tweet_np)\n",
    "        except ValueError as err: pass\n",
    "    tweets_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classes = 8\n",
    "tweets = []\n",
    "tweets_unprocessed = []\n",
    "tweets_indexed = []\n",
    "sentiments = []\n",
    "sentiments_lists = []\n",
    "tweets_list_final(os.path.join(data_path, \"data_set0.csv\"), \"utf-8-sig\")\n",
    "tweets_list_final(os.path.join(data_path, \"data_set1.csv\"), \"utf-8-sig\")\n",
    "#tweets_list_final(os.path.join(data_path, \"data_set_a0.csv\"), \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_np = np.array(tweets_indexed)\n",
    "sents_np = np.array(sentiments, np.int16)\n",
    "sents_mh_np = np.zeros((tweets_np.shape[0], classes), np.bool)\n",
    "sents_sc_np = np.zeros((tweets_np.shape[0], classes), np.bool)\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    sents_mh_np[i, sentiments_lists[i]] = 1\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    sc_indices = sentiments_lists[i]\n",
    "    sents_sc_np[i, sc_indices] = 1/len(sc_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unmatched_words=tweets_np==-1\n",
    "all_words=tweets_np>=-1\n",
    "print('unmatched words:', round(np.count_nonzero(unmatched_words)/np.count_nonzero(all_words), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unknown_words = set()\n",
    "unknown_words_l = []\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    for j in range(tweets_np.shape[1]):\n",
    "        if tweets_np[i,j]==-1:\n",
    "            unknown_words.add(tweets[i][j])\n",
    "            unknown_words_l.append(tweets[i][j])\n",
    "print('Unknown words:')\n",
    "# unknown_words\n",
    "unknown_words_counts = []\n",
    "unknown_words_dl = list(unknown_words)\n",
    "for i in range(len(unknown_words_dl)):\n",
    "    unknown_words_counts.append([unknown_words_l.count(unknown_words_dl[i]), unknown_words_dl[i]])\n",
    "sorted(unknown_words_counts, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(assumed_max_length):\n",
    "    if np.all(tweets_np[:,i]==-2):\n",
    "        max_length=i+1; break; \n",
    "print('max_length:', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_embedded = np.zeros((tweets_np.shape[0], tweets_np.shape[1], embedding_dim))\n",
    "for i in range(tweets_np.shape[0]):\n",
    "    tweet=tweets_np[i]\n",
    "    for j in range(max_length):\n",
    "        word_index=tweets_np[i,j]\n",
    "        if word_index == -1: continue\n",
    "        if word_index == -2: break\n",
    "        tweets_embedded[i,j]=np_embedding[word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('data set size:', len(sents_np))\n",
    "print('sent counts:', np.bincount(sents_np))\n",
    "print('top 3-classses per all: {:.3}'.format(sum(sorted(np.bincount(sents_np), reverse=True)[:3])/sum(np.bincount(sents_np))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_units = 200\n",
    "num_layers = 1\n",
    "batch_size = 100\n",
    "num_steps = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(0)\n",
    "inputs = tf.placeholder(tf.float32, (batch_size, num_steps, embedding_dim))\n",
    "# targets = tf.placeholder(tf.int32, (batch_size,))\n",
    "targets_mc = tf.placeholder(tf.float32, (batch_size, classes))\n",
    "\n",
    "softmax_w = tf.Variable(tf.random_uniform((num_units, classes), 0.0001, 0.001))\n",
    "softmax_b = tf.Variable(tf.random_uniform((classes,), 0.0001, 0.001))\n",
    "\n",
    "cell = rnn.BasicLSTMCell(num_units)\n",
    "tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=.7)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "all_outputs, final_states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "outputs = all_outputs[:,-1]\n",
    "\n",
    "# targets_oh = tf.one_hot(targets, classes, on_value=1, off_value=0)\n",
    "\n",
    "logits = tf.matmul(outputs, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)\n",
    "# probs = tf.nn.sigmoid(logits)\n",
    "# results = tf.cast(tf.nn.top_k(probs, 3).indices, tf.int32)\n",
    "results = tf.nn.top_k(probs, 3).indices\n",
    "# results = probs>0.5\n",
    "\n",
    "# losses=tf.reduce_sum(tf.square(tf.subtract(tf.cast(targets_oh, tf.float16), tf.cast(probs, tf.float16))))\n",
    "# losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits)\n",
    "output_near_to_right = tf.reduce_sum(tf.minimum(probs,targets_mc),1) / tf.count_nonzero(targets_mc,dtype=tf.float32)\n",
    "losses = -tf.reduce_sum(targets_mc * tf.log(tf.clip_by_value(probs, 1e-10, 1.0)),1) * (2-output_near_to_right) \n",
    "# losses=tf.reduce_sum(tf.square(tf.subtract(targets_mc, probs)))\n",
    "# losses = tf.nn.softmax_cross_entropy_with_logits(labels=targets_mc, logits=logits)\n",
    "\n",
    "opt = tf.train.AdamOptimizer()\n",
    "opt_op = opt.minimize(losses)\n",
    "\n",
    "# fw=tf.summary.FileWriter('TFSums', tf.get_default_graph())\n",
    "# fw.flush()\n",
    "# graph_runs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -tf.reduce_sum(targets_mc * tf.log(tf.clip_by_value(probs, 1e-10, 1.0)),1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iters=100; max_train=1200; max_test = 300;\n",
    "start_time = time.time()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()); total_iters=0\n",
    "total_iters+=iters\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, os.path.join(data_path, \"d\"+ str(embedding_dim), \"_word_embedding\", \"TF_Variables\")\n",
    "\n",
    "# train_baseline=np.round(np.max(np.bincount(sents_np[:max_train]))/max_train, 3)\n",
    "# test_baseline=np.round(np.max(np.bincount(sents_np[max_train:max_train+max_test]))/max_test, 3)\n",
    "train_baseline = np.round(sum(sorted(np.bincount(sents_np[:max_train]), reverse=True)[:3])/max_train,3 )\n",
    "test_baseline = np.round(sum(sorted(np.bincount(sents_np[max_train:max_train+max_test]), reverse=True)[:3])/max_test,3)\n",
    "x_axis=[]\n",
    "\n",
    "all_accs = np.zeros(iters)\n",
    "all_accs_sampled = np.zeros(iters//5+1)\n",
    "tests_accs = np.zeros(iters//5+1)\n",
    "for i in range(iters):\n",
    "    checkpoint = i if i%5 == 0 or i==iters-1 else None\n",
    "    iter_train_result = np.zeros((max_train, classes), np.bool); final_train_probs = np.zeros((max_train, classes))\n",
    "    iter_test_result = np.zeros((max_test, classes), np.bool); final_test_probs = np.zeros((max_test, classes))\n",
    "    for j in range(int(max_train/batch_size)):\n",
    "        np_inputs = tweets_embedded[j*batch_size:(j+1)*batch_size, :num_steps]\n",
    "        np_targets = sents_sc_np[j*batch_size:(j+1)*batch_size]\n",
    "        if checkpoint and i==iters-1:\n",
    "            _, np_probs, np_results = sess.run([opt_op, probs, results], feed_dict={inputs: np_inputs, targets_mc: np_targets})\n",
    "#             iter_train_result[j*batch_size:(j+1)*batch_size] = np_results\n",
    "#             iter_train_result[j*batch_size:(j+1)*batch_size] = np.eye(classes)[np_results.squeeze()] #one_hot\n",
    "            iter_train_result[np.arange(j*batch_size,(j+1)*batch_size)[:, np.newaxis], np_results] = True\n",
    "            final_train_probs[j*batch_size:(j+1)*batch_size] = np_probs\n",
    "        elif checkpoint:\n",
    "            _, np_results = sess.run([opt_op, results], feed_dict={inputs: np_inputs, targets_mc: np_targets})\n",
    "#             iter_train_result[j*batch_size:(j+1)*batch_size] = np_results\n",
    "#             iter_train_result[j*batch_size:(j+1)*batch_size] = np.eye(classes)[np_results.squeeze()]\n",
    "            iter_train_result[np.arange(j*batch_size,(j+1)*batch_size)[:, np.newaxis], np_results] = True\n",
    "        else:\n",
    "            _ = sess.run([opt_op], feed_dict={inputs: np_inputs, targets_mc: np_targets})\n",
    "\n",
    "    if checkpoint is None : continue\n",
    "    if i==iters-1: checkpoint+=1\n",
    "    checkpoint//=5\n",
    "#     checks = iter_train_result==sents_np[:max_train] \n",
    "#     checks = np.fromiter(map(lambda i,j :i in j, iter_train_result, sentiments_lists[:max_train]), dtype=np.bool)\n",
    "#     checks = np.logical_not(np.logical_xor(iter_train_result, sents_mh_np[:max_train]))\n",
    "    checks_parameter = np.count_nonzero(sents_mh_np[: max_train],1)\n",
    "    checks = np.count_nonzero(np.logical_and(iter_train_result, sents_mh_np[: max_train]),1)/ checks_parameter\n",
    "\n",
    "                \n",
    "#     all_accs[i] = np.count_nonzero(checks)/checks.size\n",
    "    all_accs[i] = np.sum(checks)/ max_train\n",
    "    \n",
    "    #update plot\n",
    "    all_accs_sampled[checkpoint] = all_accs[i]\n",
    "    x_axis.append(checkpoint*5)\n",
    "#     sys.stdout.write('\\r'); sys.stdout.write('      '); sys.stdout.write('\\r')\n",
    "#     print(i, end='')\n",
    "    plt.gca().cla() \n",
    "    plt.plot(x_axis,all_accs_sampled[:(checkpoint)+1], label=\"train(bl=\" + str(train_baseline) + \")\")\n",
    "    for test in range(int(max_test/batch_size)):\n",
    "        np_test_inputs = tweets_embedded[max_train + (test*batch_size):max_train + ((test+1)*batch_size), :num_steps]\n",
    "        np_test_targets = sents_sc_np[max_train + (test*batch_size):max_train + ((test+1)*batch_size)]\n",
    "        if i==iters-1:\n",
    "            np_test_results, np_test_probs = sess.run([results, probs], feed_dict={inputs:  np_test_inputs, targets_mc: np_test_targets})\n",
    "            final_test_probs[(test*batch_size):((test+1)*batch_size)] = np_test_probs\n",
    "        else:\n",
    "            np_test_results, = sess.run([ results], feed_dict={inputs:  np_test_inputs, targets_mc: np_test_targets})\n",
    "\n",
    "        iter_test_result[np.arange((test*batch_size),((test+1)*batch_size))[:, np.newaxis], np_test_results] = True\n",
    "    checks_parameter_test = np.count_nonzero(sents_mh_np[max_train : max_train + max_test],1)\n",
    "    checks_test = np.count_nonzero(np.logical_and(iter_test_result, sents_mh_np[max_train : max_train + max_test]),1)/ checks_parameter_test\n",
    "\n",
    "#    checks = np.any(np.logical_and(iter_test_result, sents_mh_np[max_train:max_train + max_test]), 1) * check_p\n",
    "                            \n",
    "#     tests_accs[checkpoint] = np.count_nonzero(checks_test)/checks_test.size\n",
    "    tests_accs[checkpoint] = np.sum(checks_test) / checks_test.size\n",
    "    plt.plot(x_axis,tests_accs[:(checkpoint)+1],label=\"test(bl=\" + str(test_baseline) + \")\")\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "# sys.stdout.write('\\r'); sys.stdout.write('      '); sys.stdout.write('\\r')\n",
    "\n",
    "end_time=time.time()\n",
    "winsound.Beep(2500,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " tf.count_nonzero(np_targets < x)\n",
    "# sess.run([y], feed_dict={x:x,np_targets:np_targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum((np.any(np.logical_and(iter_test_result, sents_mh_np[max_train:max_train + max_test]),1)* (np.count_nonzero(np.logical_and(iter_test_result, sents_mh_np[max_train:max_train + max_test]),1)/3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(sents_mh_np[:max_train],1)\n",
    "np.sum(np.count_nonzero(np.logical_and(iter_test_result, sents_mh_np[max_train : max_train + max_test]),1)/ checks_parameter)/ max_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum((np.count_nonzero(np.logical_and(iter_test_result, sents_mh_np[max_train : max_train + max_test]),1)/np.count_nonzero(sents_mh_np[max_train : max_train + max_test],1)))/ max_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('train_acc:', all_accs_sampled[-1], '  test_acc:', tests_accs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('top train_acc:', max(all_accs_sampled), '  top test_acc:', max(tests_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ress_std=np.std(iter_test_result, 0)\n",
    "ress_std_av=np.sum(ress_std)/ress_std.size\n",
    "print('standard deviation:', ress_std_av)\n",
    "print(\"execution time: {:.9f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Wrong results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_map=['Happy','Love','Hopeful','Neutral','Angry','Hopeless','Hate','Sad']\n",
    "for i in range(max_test):\n",
    "          if checks[i]==False:\n",
    "            tweet_p=' '.join(tweets[max_train+i])\n",
    "            res_p=[sent_map[l] for l in np.nonzero(iter_test_result[i])[0]]\n",
    "            targets_p=[sent_map[j] for j in sentiments_lists[max_train+i]]\n",
    "            probs_p=[float('{:.3}'.format(k)) for k in final_test_probs[i]]\n",
    "            print(tweet_p, '\\n>' , res_p, \">>\", targets_p, '>>>', probs_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "unprocessed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sent_map=['Happy','Love','Hopeful','Neutral','Angry','Hopeless','Hate','Sad']\n",
    "for i in range(max_test):\n",
    "          if checks[i]==False:\n",
    "            tweet_p=tweets_unprocessed[max_train+i]\n",
    "            res_p=[sent_map[l] for l in np.nonzero(iter_test_result[i])[0]]\n",
    "            targets_p=[sent_map[j] for j in sentiments_lists[max_train+i]]\n",
    "            probs_p=[float('{:.3}'.format(k)) for k in final_test_probs[i]]\n",
    "            print(tweet_p, '\\n>' , res_p, \">>\", targets_p, '>>>', probs_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checks_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_map=['Happy','Love','Hopeful','Neutral','Angry','Hopeless','Hate','Sad']\n",
    "for i in range(max_test):\n",
    "            \n",
    "    if checks_test[i]==False:\n",
    "        tweet_p=tweets_unprocessed[max_train+i]\n",
    "        res_p=[sent_map[l] for l in np.nonzero(iter_test_result[i])[0]]\n",
    "        targets_p=[sent_map[j] for j in sentiments_lists[max_train+i]]\n",
    "        probs_p=[float('{:.3}'.format(k)) for k in final_test_probs[i]]\n",
    "        print(tweet_p, '\\n>' , res_p, \">>\", targets_p, '>>>', probs_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(data_path, 'Sessions', 'ta716_736_model_si_softmax_top3acc_200u1l', 'ta716_736_model_si_softmax_top3acc_200u1l'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
